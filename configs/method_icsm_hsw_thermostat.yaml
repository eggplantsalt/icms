vla_path: openvla/openvla-7b
cache_dir: /opt/data/private/openvla_icms/hf_cache
data_root_dir: /opt/data/private/modified_libero_rlds
run_root_dir: /opt/data/private/openvla_icms/runs
adapter_tmp_dir: /opt/data/private/openvla_icms/tmp
artifact_dir: /opt/data/private/openvla_icms/artifacts
probe_root_dir: /opt/data/private/modified_libero_rlds
tmp_dir: /opt/data/private/openvla_icms/tmp

# Training data (update to your actual dataset name)
dataset_name: libero_spatial_no_noops
batch_size: 2
max_steps: 500
save_steps: 250
merge_lora_during_training: false
learning_rate: 9e-5
min_learning_rate: 5e-5
lr_scheduler: cosine
lr_warmup_steps: 20
grad_accumulation_steps: 8
image_aug: true
shuffle_buffer_size: 2000

# LoRA
use_lora: true
lora_rank: 16
lora_dropout: 0.0
use_quantization: false

use_wandb: false
run_id_note: method

# Resume
resume_adapter_dir: /opt/data/private/openvla_icms/tmp/openvla-7b+libero_spatial_no_noops+b32+lr-0.0002+lora-r16+dropout-0.0--method--image_aug
resume_global_step: 250

# Method
method_enabled: true
icms_artifact_dir: /opt/data/private/openvla_icms/artifacts/icms_openvla-7b
hsw_beta: 1.0
hsw_gamma: 1.0
hsw_eps: 1e-8
prompt_template: "In: {instruction}\nOut:"

# Probe
probe_dataset_name: libero_spatial_no_noops
probe_batch_size: 4

thermostat_update_interval: 5
thermostat_warmup_steps: 50
thermostat_min_beta: 0.2
thermostat_max_beta: 0.8
thermostat_min_gamma: 0.1
thermostat_max_gamma: 0.8
thermostat_k_beta: 3.0
thermostat_k_gamma: 3.0

# HSW layer selection (use last 2 layers)
hsw_layer_ids: [-2, -1]
